{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing Brown\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from spacy.en import English\n",
    "\n",
    "brown_sents = [unicode(' '.join(sent)) for sent in brown.sents()]\n",
    "\n",
    "def parse(sents, n_threads=20, batch_size=1000):\n",
    "    size = len(sents)\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    data, idx2word, idx2tag = [], set(), set()\n",
    "    parser = English()\n",
    "    for parsed_sent in parser.pipe(sents, n_threads=n_threads, batch_size=batch_size):\n",
    "        count += 1\n",
    "        X_i,Y_i = [], []\n",
    "        for token in parsed_sent:\n",
    "            X_i.append(token.lemma_)\n",
    "            Y_i.append(token.pos_)\n",
    "            idx2word.add(token.lemma_)\n",
    "            idx2tag.add(token.pos_)\n",
    "        data.append((X_i,Y_i))\n",
    "        if count%(size//20)==0:\n",
    "            end_time = time.time()\n",
    "            print '... parsed', count, 'sents' + '('+str(end_time-start_time)+')'\n",
    "            start_time = end_time\n",
    "    return data, list(idx2word), list(idx2tag), count\n",
    "\n",
    "data, idx2word, idx2tag, count = parse(brown_sents)\n",
    "word2idx = {w:i for i,w in enumerate(idx2word)}\n",
    "tag2idx = {t:i for i,t in enumerate(idx2tag)}\n",
    "VOCAB_SIZE = len(idx2word)\n",
    "TAG_SIZE = len(idx2tag)\n",
    "MAX_LEN = np.array([len(sent) for sent in data]).max()\n",
    "print\n",
    "print 'STATS:'\n",
    "print 'data size:', count\n",
    "print 'vocab size:', VOCAB_SIZE\n",
    "print 'tag size:', TAG_SIZE\n",
    "\n",
    "data_code = []\n",
    "for X_i,Y_i in data:\n",
    "    data_code.append(([word2idx[w] for w in X_i],[tag2idx[t] for t in Y_i]))\n",
    "    \n",
    "split = 0.8\n",
    "cutoff = int(count*split)\n",
    "data_train, data_test = data_code[:cutoff], data_code[cutoff:]\n",
    "print 'train size:', len(data_train)\n",
    "print 'test size:', len(data_test)\n",
    "\n",
    "''' Data Stats\n",
    "\n",
    "STATS:\n",
    "data size: 57340\n",
    "vocab size: 35241\n",
    "tag size: 15\n",
    "train size: 45872\n",
    "test size: 11468\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "''' Data Format\n",
    "\n",
    "# original\n",
    "([u'the', u'fulton', u'county', u'grand', u'jury', u'say', u'friday', u'an', u'investigation',\n",
    "u'of', u'atlanta', u\"'s\", u'recent', u'primary', u'election', u'produce', u'``', u'no', u'evidence',\n",
    "u\"''\", u'that', u'any', u'irregularity', u'take', u'place', u'.'], \n",
    "[u'DET', u'PROPN', u'PROPN', u'PROPN', u'PROPN', u'VERB', u'PROPN', u'DET', u'NOUN', u'ADP',\n",
    "u'PROPN', u'PART', u'ADJ', u'NOUN', u'NOUN', u'VERB', u'PUNCT', u'DET', u'NOUN', u'PUNCT', \n",
    "u'ADJ', u'DET', u'NOUN', u'VERB', u'NOUN', u'PUNCT'])\n",
    "\n",
    "# encoded\n",
    "([11414, 29182, 13262, 3664, 18109, 27138, 7843, 9840, 18259, 16419, 10809, 18036, 10297, \n",
    "5613, 6999, 2859, 9415, 15909, 11514, 18019, 29312, 16179, 34814, 9580, 24917, 375], \n",
    "[5, 4, 4, 4, 4, 14, 4, 5, 1, 2, 4, 9, 13, 1, 1, 14, 8, 5, 1, 8, 13, 5, 1, 14, 1, 8])\n",
    "\n",
    "'''\n",
    "\n",
    "# Setup DyNet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import dynet as dn\n",
    "dp = dn.DynetParams()\n",
    "dp.set_mem(4000)\n",
    "dp.set_autobatch(True)\n",
    "\n",
    "class DataIterator:\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.size = len(data)\n",
    "        self.epoch = 0\n",
    "        self.cursor = 0\n",
    "        \n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n > self.size:\n",
    "            self.epoch += 1\n",
    "            self.cursor = 0\n",
    "        batch = self.data[self.cursor:self.cursor+n]\n",
    "        self.cursor += n\n",
    "        return batch\n",
    "\n",
    "def train(net, train, test, epochs=1):\n",
    "\n",
    "    train = train*epochs\n",
    "    train_size = len(train)\n",
    "    test_freq = 10\n",
    "    optimizer = dn.AdamTrainer(net.model)\n",
    "    test_losses, iterations = [], []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "# # Iterating feed    \n",
    "#     gen = DataIterator(train)\n",
    "#     batch_count = 0\n",
    "#     while gen.epoch < epochs:\n",
    "#         dn.renew_cg()\n",
    "#         batch_count += 1\n",
    "#         batch = gen.next_batch(50)\n",
    "#         losses = []\n",
    "#         for i,(X_i,Y_i) in enumerate(batch):\n",
    "#             loss = net.get_loss(X_i,Y_i)\n",
    "#             losses.append(loss)\n",
    "#         loss = dn.esum(losses)\n",
    "#         loss.backward()\n",
    "#         optimizer.update()\n",
    "        \n",
    "#         if batch_count%test_freq==0:\n",
    "#             test_loss = np.array([net.get_loss(X_i,Y_i).value() \n",
    "#                                   for X_i,Y_i in test]).sum()\n",
    "#             print '... loss:', test_loss\n",
    "#             test_losses.append(test_loss)\n",
    "#             iterations.append(batch_count)            \n",
    "            \n",
    "    for i,(X_i,Y_i) in enumerate(train):\n",
    "        loss = net.get_loss(X_i,Y_i)\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        end_time = time.time()\n",
    "        print '... train loss:', loss.value(), '('+str(end_time-start_time)+')'\n",
    "        start_time = end_time\n",
    "        if i%test_freq==0:\n",
    "            test_loss = np.array([net.get_loss(X_i,Y_i).value() \n",
    "                                  for X_i,Y_i in test]).sum()\n",
    "            print '... loss:', test_loss\n",
    "            test_losses.append(test_loss)\n",
    "            iterations.append(i//test_freq)\n",
    "    \n",
    "    plt.plot(iterations, test_losses)\n",
    "    plt.axis([0, 100, 0, len(test)*MAX_LEN])\n",
    "    plt.show()\n",
    "    print 'loss on test:', test_losses[-1]\n",
    "    \n",
    "    \n",
    "class SRN:\n",
    "    \n",
    "    def __init__(self, num_layers, embeddings_size, hidden_size):\n",
    "        self.model = dn.Model()\n",
    "        self.embeddings = self.model.add_lookup_parameters((VOCAB_SIZE, embeddings_size))\n",
    "        self.RNN = dn.LSTMBuilder(num_layers, embeddings_size, hidden_size, self.model)\n",
    "        self.output_W = self.model.add_parameters((VOCAB_SIZE, hidden_size))\n",
    "        self.output_b = self.model.add_parameters((VOCAB_SIZE))\n",
    "        \n",
    "    def embed(self, X):\n",
    "        return [self.embeddings[w_i] for w_i in X]\n",
    "    \n",
    "    def run(self, init_state, X_emb):\n",
    "        st = init_state\n",
    "        states = st.add_inputs(X_emb)\n",
    "        return [s.output() for s in states]\n",
    "    \n",
    "    def get_probs(self, rnn_output):\n",
    "        output_W = dn.parameter(self.output_W)\n",
    "        output_b = dn.parameter(self.output_b)\n",
    "        return dn.softmax(output_W*rnn_output + output_b)\n",
    "    \n",
    "    def get_loss(self, X, Y):\n",
    "        dn.renew_cg()\n",
    "        X_emb = self.embed(X)\n",
    "        init_state = self.RNN.initial_state()\n",
    "        rnn_outputs = self.run(init_state, X_emb)\n",
    "        losses = []\n",
    "        for rnn_output,Y_i in zip(rnn_outputs,Y):\n",
    "            probs = self.get_probs(rnn_output)\n",
    "            losses.append(-dn.log(dn.pick(probs,Y_i)))\n",
    "        return dn.esum(losses)\n",
    "    \n",
    "    def predict_tag(self, probs):\n",
    "        probs = probs.value()\n",
    "        return idx2tag[probs.index(max(probs))]\n",
    "            \n",
    "    def predict(self, X):\n",
    "        dn.renew_cg()\n",
    "        X_emb = self.embed(X)\n",
    "        init_state = self.RNN.initial_state()\n",
    "        rnn_outputs = self.run(init_state, X_emb)\n",
    "        pred = []\n",
    "        for rnn_output in rnn_outputs:\n",
    "            probs = self.get_probs(rnn_output)\n",
    "            pred.append(self.predict_tag(probs))\n",
    "        return pred\n",
    "\n",
    "# Training\n",
    "    \n",
    "num_layers = 1\n",
    "embeddings_size = 50\n",
    "hidden_size = 50\n",
    "\n",
    "rnn = SRN(num_layers, embeddings_size, hidden_size)\n",
    "train(rnn, data_train, data_test)\n",
    "print 'Example:'\n",
    "print 'Input sent:', [idx2word[idx] for idx in [w_i for w_i,t_i in data_test[0]]]\n",
    "print 'Predicted tags:', rnn.predict([w_i for w_i,t_i in data_test[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
